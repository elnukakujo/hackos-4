{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "class SingleCellPerturbationMultiModalModel(nn.Module):\n",
    "    def __init__(self, output_dim=5, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load ChemBERTa embeddings\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\", is_decoder=True)\n",
    "        chemberta_embeddings = model.roberta.embeddings\n",
    "        self.smiles_embedding = chemberta_embeddings\n",
    "        for param in self.smiles_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        embedding_dim = self.smiles_embedding.word_embeddings.embedding_dim\n",
    "        \n",
    "        # 1D CNN components\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim + 6,  # Match combined feature dimension\n",
    "                out_channels=64,\n",
    "                kernel_size=9,\n",
    "                padding=4  # Maintain sequence length\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.8)\n",
    "        )\n",
    "        \n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 64))  # Match CNN output channels\n",
    "        \n",
    "        # Transformer encoder (single layer)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=64,  # Match CNN output channels\n",
    "                nhead=num_heads,\n",
    "                dropout=0.8,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=1\n",
    "        )\n",
    "        \n",
    "        # Regression head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, cell_type):\n",
    "        # Get SMILES embeddings\n",
    "        smile_emb = self.smiles_embedding(input_ids)\n",
    "        \n",
    "        # Prepare cell type features\n",
    "        cell_type_emb = cell_type.unsqueeze(1).repeat(1, smile_emb.size(1), 1)\n",
    "        \n",
    "        # Combine features and apply CNN\n",
    "        combined = torch.cat((smile_emb, cell_type_emb), dim=-1)\n",
    "        combined = combined.permute(0, 2, 1)  # [batch, features, seq_len] for Conv1d\n",
    "        conv_out = self.conv_block(combined).permute(0, 2, 1)  # Back to [batch, seq_len, features]\n",
    "        \n",
    "        # Add [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(conv_out.size(0), -1, -1)\n",
    "        inputs = torch.cat([cls_tokens, conv_out], dim=1)\n",
    "        \n",
    "        # Update attention mask for [CLS]\n",
    "        mask = torch.cat([torch.ones_like(attention_mask[:, :1]), attention_mask], dim=1)\n",
    "        \n",
    "        # Transformer processing\n",
    "        out = self.encoder(\n",
    "            inputs,  # Transformer expects [seq_len, batch, features]\n",
    "            src_key_padding_mask=~mask.bool()\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for regression\n",
    "        return self.head(out[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(df, svd):\n",
    "    \"\"\"Preprocess gene expression data with normalization and SVD\"\"\"\n",
    "    # Create a copy of the gene expression columns to avoid SettingWithCopyWarning\n",
    "    gene_data = df.loc[:, 'A1BG':].copy()\n",
    "    \n",
    "    # Normalize each column between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    gene_data_normalized = scaler.fit_transform(gene_data)\n",
    "    \n",
    "    # Apply SVD transformation\n",
    "    gene_expression_reduced = svd.fit_transform(gene_data_normalized)  # Shape: (num_samples, num_components)\n",
    "    \n",
    "    # Create new DataFrame with the required columns\n",
    "    processed_df = pd.DataFrame({\n",
    "        'cell_type': df['cell_type'].values,\n",
    "        'SMILES': df['SMILES'].values,\n",
    "        'gene_expressions': [list(row) for row in gene_expression_reduced]\n",
    "    })\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from rdkit import Chem\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SingleCellPerturbationDataset(Dataset):\n",
    "    def __init__(self, smiles_list, cell_types, targets, augment_prob=0.8):  # Higher augmentation\n",
    "        self.smiles_list = smiles_list\n",
    "        self.augment_prob = augment_prob\n",
    "        \n",
    "        # Convert targets to torch tensor first\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "        \n",
    "        # Cell type encoding (now handles numpy/pandas inputs)\n",
    "        cell_types_reshaped = cell_types.reshape(-1, 1) if isinstance(cell_types, np.ndarray) else cell_types.values.reshape(-1, 1)\n",
    "        self.cell_types = torch.tensor(\n",
    "            OneHotEncoder(sparse_output=False).fit_transform(cell_types_reshaped),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Target standardization (using torch operations)\n",
    "        self.target_mean = torch.mean(targets_tensor, dim=0)\n",
    "        self.target_std = torch.std(targets_tensor, dim=0)\n",
    "        self.targets = (targets_tensor - self.target_mean) / (self.target_std + 1e-8)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "        \n",
    "    def augment_smiles(self, smiles):\n",
    "        \"\"\"More robust augmentation with coordinate randomization\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return smiles\n",
    "                \n",
    "            # Generate different 2D coordinates\n",
    "            Chem.rdDepictor.Compute2DCoords(mol)\n",
    "            return Chem.MolToSmiles(mol, doRandom=True, isomericSmiles=True, canonical=False)\n",
    "        except:\n",
    "            return smiles\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Required by DataLoader - returns number of samples\"\"\"\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = str(self.smiles_list[idx])\n",
    "        if random.random() < self.augment_prob:\n",
    "            smiles = self.augment_smiles(smiles)\n",
    "            \n",
    "        encoded = self.tokenizer(smiles, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=100)\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # [seq_len]\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # [seq_len]\n",
    "            \"cell_type\": self.cell_types[idx],  # [6]\n",
    "            \"target\": self.targets[idx]  # [100]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_rowwise_rmse(preds: np.ndarray, targets: np.ndarray) -> float:\n",
    "    rowwise_mse = np.mean((preds - targets) ** 2, axis=1)  # Compute MSE per row\n",
    "    rowwise_rmse = np.sqrt(rowwise_mse)  # Convert to RMSE\n",
    "    return np.mean(rowwise_rmse)  # Average across all rows\n",
    "\n",
    "class MeanRowwiseRMSELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # Small value to prevent numerical issues\n",
    "\n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        rowwise_mse = torch.mean((preds - targets) ** 2, dim=1)  # Compute MSE per row\n",
    "        rowwise_rmse = torch.sqrt(rowwise_mse + self.epsilon)  # Convert to RMSE\n",
    "        return torch.mean(rowwise_rmse)  # Average over all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Disable gradient computation during testing (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            cell_type = batch[\"cell_type\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            target = batch[\"target\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(input_ids, attention_mask, cell_type)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            predictions.append(output.cpu().numpy())  # Moving output to CPU for storing as numpy\n",
    "            true_labels.append(target.cpu().numpy())\n",
    "\n",
    "    # Convert predictions and true labels to numpy arrays\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Optionally, compute loss or other metrics on the test set\n",
    "    # For example, using Mean Rowwise Root Mean Squared Error on the predictions:\n",
    "    return mean_rowwise_rmse(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Test Mean Rowwise RMSE Loss: 0.5682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "df_test = pd.read_parquet(\"dataset/de_test_split.parquet\")\n",
    "\n",
    "# Preprocess the data with normalization and SVD of the gene expressions\n",
    "svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "df_test = preprocess(df_test, svd)\n",
    "\n",
    "test_dataset = SingleCellPerturbationDataset(\n",
    "    smiles_list=df_test['SMILES'].to_numpy(),\n",
    "    cell_types=df_test['cell_type'].to_numpy(),\n",
    "    targets=np.array(df_test['gene_expressions'].tolist(), dtype=np.float32),\n",
    "    augment_prob=0.0  # No augmentation for test set\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 4\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    " # Initialize model,and load weights\n",
    "model = SingleCellPerturbationMultiModalModel(output_dim=5).to(device)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "test_loss = eval_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Test Mean Rowwise RMSE Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
